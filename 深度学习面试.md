# 深度学习面试

### 1. Overfitting 怎么解决

dropout, cross validation, regularisation, Batch Normalisation

dropout: 相当于bagging的过程，在训练过程中按照概率p只使用一部分特征进行训练，测试时需要将输出结果乘以p使下一层输入规模保持不变。

cross validation: 交叉验证，0.2-0.33，一部分作为验证集，一轮epoch完了之后，把样本打乱，再继续训练。一般不使用k-折：k折是指将训练数据集分为k个子集，一个做测试，剩下子集轮流与它比较模型，直至所有均为使用。但是每次做这样的过程对于深度学习来说时间成本太大了。

BN:优点：神经网络需要归一化来提高泛化能力，其次如果分布各不相同，每次迭代需要适应不同的分布，会降低网络训练速度。均值为0方差为1。对上层数据归一化到0中心的分步，之后利用两个参数，来学习恢复出原始的某一层所学到的特征的。快速收敛，可以使用较大的初始学习率。提高网络泛化。进行的归一化，防止梯度弥散，通过将activation规范为均值和方差一致的手段，使得原来会减小的activation的scale变大。每个特征图只有2个可学习的参数，进行变换重构，用以恢复原始的某一层所学到的特征分布。2\*depth。破坏原来的数据分布，一定程度上缓解过拟合。

Batch Normalization的缺点：Batch Normalization不适用于对噪声敏感的强化学习、生成模型。额外的存储空间来保存mini batch的均值和方差。高度依赖于mini-batch的大小，不适合在线学习，不适合RNN因为RNN是变长的，不是定长的。

BN应该在激活函数前，在CNN时应该在卷积之后，relu激活函数之前。在CNN时的BN将一个feature map视为一个神经元，每个的均值是特征图的所有神经元的平均值和方差

权重衰减：在对loss function中加入一些正则项，在sgd中添加一项。

### 2. CNN中的卷积池化怎么理解

input-&gt;\[\[conv -&gt; relu\]\*N -&gt; Pool\]\*M-\[FC-&gt;RELU\]\*K -.FC

CNN使用场景：可以降采样但任然可以保持局部信息，权值共享保证高效，

CNN采用Relu是因为，在各个不同卷积的权重不相同，如果采用了sigmoid作为激活函数，会导致连乘过程的梯度消失，采用了Relu之后，由于导数变为1所以可以缓解梯度消失的情况。

Convolution Kernel 具有的一个属性就是局部性。即它只关注局部特征，局部的程度取决于 Convolution Kernel 的大小。卷积核size设为奇数时，padding的时候图像两边依然对称，保证锚点在中心，方便以模块中心为标准进行滑动卷积，做padding主要是为了保证卷积之后的图像尺寸大小不会减少的太快。

一个 Convolution Kernel 在与 Input 不同区域做卷积时，它的参数是固定不变的。放在 DNN 的框架中理解，就是对同一层 Layer 中的神经元而言，它们的 w 和 b 是相同的，只是所连接的节点在改变。因此在 CNN 里，这叫做 Shared Weights and Biases。权值不需要提前设计。多个kernel可以提取出多个不同的特征。这里的共享权重的意思是指在一个kernel的权重在扫的时候是共享的。步长代表移动位数，padding填充的位数。通过将输入图像的局部区域拉伸成列，这个局部区域大小与单个卷积核大小一致，这个局部区域重复做到feature map size次。卷积层的权重也同样被拉伸成列。CNN有平移不变性

计算output的dimension问题？？？尺寸不整除，卷积向下取整，池化向上取整。

feature map: 

\[ \(image\_W - kernel\_size + 2\*P\)/Stride +1, \(image\_H - kernel\_size + 2\*P\)/Stride+1\]\*number\_of\_kernel

参数个数\(kernel\*kernel\_size\*kernel\_depth+1\)\*number\_kernel

池化：其实是对数据的下采样，减少参数，有平移不变性，使得网络的鲁棒性增强了，有一定抗扰动的作用。Pooling 对于输入的 Feature Map，选择某种方式对其进行压缩。maxPooling 有旋转不变性

feature map: F\*F filter size

\[feature\_map1-F\]/S+1

参数个数：depth\_feature\_map \* \(1+1\)

作用：1 其中一个显而易见，就是减少参数。通过对 Feature Map 降维，有效减少后续层需要的参数 2 另一个则是 Translation Invariance。它表示对于 Input，当其中像素在邻域发生微小位移时，Pooling Layer 的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用

max pooling:希望得到强特征，文本分类中常用，保留图片纹理信息，在反向传播时，只对max那个点回传梯度。

average pooling:用于主题，考虑一段文本可能会有多个主题，可以广泛反应特征，保留背景信息。反向传播的梯度除以kernel size得到的梯度分给每一块。

k-max pooling: 取一个区域前k个大的特征，保留了相对的位置信息

全连接层可在模型表示能力迁移过程中充当防火墙的作用。在卷积层之后加入fc将学到的分布式特征表示映射到样本标记空间的作用。本质是有一个特征空间变换到另一个特征空间。

### 3. RNN LSTM GRU

RNN的参数共享出现体现在时间步上参数共享。

由于RNN在各个时间步上共享同一个参数W，那么在即使采用了Relu作为激活函数，求导过程还是存在对W的连乘操作，无法避免梯度消失。

参数计算：\(number\_feature\*unit\)+unit\*unit+unit

RNN主要是为了考虑时间序列对最后结果的影响。想要catch整个句子所有词对最后结果的影响。但是由于其在计算梯度时每个词对前一个是累乘的影响，这就会导致在反向传播的时候造成梯度爆炸或者梯度消失的影响。在计算损失函数对hidden_state权重的导数时，当前时间步下ht对权重的导数为时间步k=1至k=t对权重的导数之和乘上ht对hk的导数，此时ht对hk的导数即为一个累乘，累乘导致梯度爆炸和消失。梯度大于1和小于1时。可以采用梯度截断的方式来进行缓解。在反向传播时，在某一序列位置t的梯度损失由当前位置的输出对应的梯度损失和序列索引位置t+1时的梯度损失两部分共同决定。

LSTM，GRU则是采用了gate的机制，令梯度的影响从累乘变成了累加。从而可以避免出现梯度爆炸或者梯度消失。实质上其实就是在计算上一层信息对当层信息导数时，rnn只是普通的连乘的形式，而在LSTM GRU中，上一层信息对当层信息的影响，不是只有一项，而是有多项，形式上为加法的形式。

LSTM: input, forget, output gate + cell 存储当前信息。forget控制引入多少上一层cell信息，input用sigmod控制引入多少当前信息，forget和input线形组合形成当前cell信息，output控制输出多少当前cell信息作为hidden state。计算cell state的导数是，当forget gate值接近1，就可以避免梯度消失问题。

GRU与LSTM相比，少了一个gate,由此就少了一些矩阵乘法，GRU虽与LSTM的效果较近，但却因此在训练中节省了很多时间。Reset, Update gate. Reset: 控制使用多少比例的上一层hidden state， update gate: 使用多少临时信息和上一层hidden state。 update 相当于LSTM中的forget 和input

### 3.2 RBM受限玻尔兹曼机

两层网络: hidden layer 和visible layer，有权重网络W，偏移向量a和b，神经元取值为0或者1

### 4. 激活函数

全连接层以sigmoid和tanh为常见，卷积常用relu

特点：**非线性。可微性：** 当优化方法是基于梯度的时候，这个性质是必须的。**单调性：** 当激活函数是单调的时候，单层网络能够保证是凸函数。**f\(x\)≈x：** 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。**输出值的范围：** 当激活函数输出值是 **有限** 的时候，基于梯度的优化方法会更加 **稳定**，因为特征的表示受有限权值的影响更显著

sigmoid: \[0,1\]当输入非常大或者非常小的时候（saturation），这些神经元的梯度是接近于0的。Sigmoid 的 output 不是0均值，会出现梯度一直为正或者为负的情况。

tanh: 2sigmoid\(2x\)-1. tanh 是0均值的

Relu: max\(0,x\).ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多。ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。缺点：不是零中心。某些神经元可能永远不会被激活，导致相应参数永远不会被更新。负数部分，梯度一直为0。

Leakly Relu: max\(0.01x, x\)

ELU\(\)

### 5.梯度下降优化方法

**Momentum**：SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。超参数 mu即衡量上一时刻的动量：通常设0.9。初始动量设为0.5

**Nesterov Momentum**：动量更新的变种，在计算梯度时，损失函数李考虑先去除上一时刻动量

**Adagrad:** Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理稀疏数据。 epsilon 1e-8

**RMSProp**：Adagrad的分母时累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题

**Adam**：动量和RMSProp两者的结合。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。超参数设计：beta1:0.9, beta2:0.99, epsilon 1e-8

在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop, Adam 等

_为啥RNN中一般采用RMSprop作为优化方法，而不用Adam_

### 6. 图像底层卷积结构

VGG: 严格使用3x3的过滤器\(stride =1，pad= 1\)和2x2 maxpooling层\(stride =2\).作者认为两个3x3的卷积层组合可以实现5x5的有效感受野，3个3\*3可以实现7x7的感受野。这就在保持滤波器尺寸较小的同时模拟了大型滤波器，减少了参数。此外，有两个卷积层就能够使用两层ReLU。更多的激活函数、更丰富的特征，更强的辨别能力。卷积层的参数减少。

ResNet模型是目前最好的CNN架构.错误率低。其根本逻辑在于一般来说网络越深拟合效果越好，但是容易产生一些梯度消失或者梯度爆炸问题，那么它的处理放在在激活函数或者隐藏层之后.     利用short cut的方式，跳过两层之后，最后的output=F\(x\)+x，引入两层之前的input，求导时传到前一层的梯度就变为1+...,从而达到解决梯度消失问题。这样的一个短路连接操作就被称为是一个小的residual block。这个residual block一般放在pooling之后。引入了更为丰富的参考信息或者更为丰富的维度。Resnet参考了VGG19，在使用stride=2的卷积做下采样就是池化，使用gobal average pool代替全连接层.当x和F\(x\)之间的channel不同时，则不是简单的相加，而是对x其中W是卷积操作，用来调整x的channel维度的.

### 7. RCNN, Fast RCNN, Faster RCNN, YOLO, SSD

高斯滤波可以降低图片的清晰度，sigma越大图像越模糊。对图片进行离散化窗口卷积，将中心点当作正态分布的原点，其他点按照其在正态曲线上的位置分配权重。或者均值模糊。

在RCNN和Fast RCNN中的，Selective Search 的作用是聚合2000个不同的区域，这些区域有最高的可能性会包含一个物体。在我们设计出一系列的区域建议之后，这些建议被汇合到一个图像大小的区域，能被填入到经过训练的CNN\(论文中的例子是AlexNet\)，能为每一个区域提取出一个对应的特征。

RCNN 和fast RCNN的区别就是在于，RCNN选择候选区是在原图上，而fast RCNN在特征图上,roi\_pool将每个候选去均匀分成M\*N，对每块进行max pooling。然而对ROI中的**每块候选区域**，做multi-task 的loss。RCNN需要在选择完特征区之后利用多个CNN结构，相对更加费时。

Faster RCNN就在fast RCNN的基础上利用在最后一个卷积层上引入了一个区域建议网络\(RPN\)。这一网络能够只看最后一层的特征就产出区域建议。取代了selective search的作用。在feature map上有一个anchor，m种不同框的大小尺度，n种不同框的缩放比例，所以对于每个位置都有m\*n个比较anchor

Faster RCNN的流程：得到feature map之后利用rpn层进行候选点进行分析，各个点有长宽高三种尺寸，每个大小有3中，所以对每个点9个anchor来做分类和回归。该层网络的正负样本选择，通过计算anchor和各个转化比例之后的bbox的iou，如果大于一定阈值0.7或者大于当前最佳iou就当作正样本，如果低于0。3就当作负样本。在各个类下，做非极大抑制，选择前n个作为真正候选区。 Roi pooling，将各个层输出的output尺寸保持一致。input为feature map和候选区的4个坐标点和索引。output：batch个vector，vector的尺寸为channel，w，h。根据输入的image，将候选区映射到feature map上2。将映射后的区域划分为相同大小的sections3。将每个section进行 max pooling。

使用非极大性抑制，多分类问题时根据分类器类别分类概率排序，从小到大属于某类的概率。从最大概率矩阵框开始，判断IOU重叠面积是否大于某一个设定的阈值。如果超过阈值，那么就丢弃改矩形框。对于剩下的矩形框进行同样的操作。找到所有保留下来的矩形框。

ospa:

控制参数：anchor 的尺寸和ratios，pos\_iou\_threshold，neg\_iou\_threshold

yolo: 算法首先把输入图像划分成S\*S的格子，然后对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence。其中x,y是指当前格子预测得到的物体的bounding box的中心位置的坐标。w,h是bounding box的宽度和高度。**每个bounding box都对应一个confidence score**，每个bounding box的长宽比不同，如果grid cell里面没有object的中心点，confidence就是0，如果有，则confidence score等于预测的box和ground truth的IOU值。loss function:坐标误差+IOU误差+分类误差，三者均是均方差的形式。让于groudtruth IOU最大的bbox作为predictor

测试时：每个网格预测的class信息和bounding box预测的confidence信息相乘，就是每个bbox的类别置信度

selective search:

区域合并算法，输入图片，输出物体位置的可能结果。1 获取原始分割区域，初始化相似度集合为空 2  计算两两相邻区域之间的相似度，将其添加到相似度集合中。3 从相似度结合之中找出相似度最大的区域ri和rj，合并成一个区域rt。计算rt和其他区域的相似度，将结果添加到相似度集合，将rt添加到区域集合R中 4 获得哪个区域的BB，就是物体位置可能结果L。

相似度计算上有：颜色，纹理，大小，吻合相似度

### 8. 调整参数经验

**参数初始化**

**数据预处理方式：0中心。**数据在进入深度学习之前需要做一个归一化，消除量纲的影响，因为假设如果采用的是固定学习率，而数据没归一化，那么会导致各个方向上的梯度更新不一致（因为取决于它的值），很难收敛。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。如果不归一化，出现特别大的值的特征会令激活函数饱和。

**训练技巧:** I要做梯度归一化,即算出来的梯度除以minibatch size 2. dropout对小数据防止过拟合有很好的效果,值一般设为0.5。dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置. adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数。rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好。word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果。LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果。

relu+bn

数据的shuffle 和augmentation

adam是不需要特别调lr，sgd要多花点时间调lr和initial weights

我们可以观察到误差出现了许多小的"涨落"， 不需要担心，只要在训练集和交叉验证集上有累积的下降就可以了，为了减少这些“起伏”，可以尝试增加批尺寸\(batch size\)。

考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。只对最后几层进行调参\(fine tune\)。或者利用已经训练好的模型用作以提取特征。网络靠前越local有最有的基本特征，所以前面的网络不进行调整。新的数据库小，两者相似则当作特征提取器。新的数据库大，相似度高，全局fit。新的数据库小，差异大，训练最后几层，或者从某层开始取特征。

### 9. 是否适合用深度学习

数据集足够大，数据集有局部相关性

当数据不足的时候，可以考虑使用数据增强的方法。数据增强旨在通过变换生成现有训练样本的变体，从而创建更多的训练数据，这些变换往往可以反映现实世界中会发生的变化。对于图像来说，对图像进行旋转，翻转，缩放，裁剪，平移，高斯噪声。对于自然语言处理来说，NLP中的数据是离散的。它的后果是我们无法对输入数据进行直接简单地转换。小的扰动可能会改变含义

### 10 搜索引擎预测下一个单词或者纠错

基于N-gram，即向前看N个字符的方法来基于统计建模argmax P\(wn\|w1,....wn-1\)，通过统计频次来得知各项相互转移的概率变化。预测出概率最大的几个词。

纠错时：P\(c\|W\): w is word, c means the correct or wrong。P\(c\|W\)=P\(w\|c\)\*P\(c\)

预测单词：可采用seq2seq

### 11 深度学习参数初始化

全零初始化，导致隐藏层神经元的激活值均为0，不同维度参数得到相同的更新。LR中可以全零初始化，因为它只有一个input和一个output，对于w的梯度完成取决与\(sigmoid-1\)x,所以x不同就可以令w的梯度不同。而在有两层以上mlp时，两个sigmoid举例，此时由于各层的output相同，导致在计算w的梯度更新时得到的梯度均相同，所以导致所有w均相同。除了第一层之外的梯度均相同。第一层内部均一致。也不能全部为相同的数，经实验，导致的结果和全0一样，因为都有相同的output，导致会产生相同的梯度矩阵

随即小的随机数，randn\(0,1\)不能太小，太小会导致小的梯度，也会产品梯度弥散问题。神经元的方差会随着神经元的数目增大而变多

标准初始化:\(-1/^d,1/^d\),d是每个神经元的输入数目。除以d保证有相同的分步。神经元数目计算：全连接层就是hidden layer最后一个参数，网络的神经原计算不算输入层。对于cnn就是feature map的所有相乘个神经元。对于RNN隐藏层的维度就是神经元个数与全连接层一样。

Xavier初始化：

偏置初始化通常为0，或比较小的数。

使用Relu without BN时，选用HE，初始化，将参数初始化为服从高斯分布或者均匀分步较小的随机数。

使用BN时，减少了网络对参数初始值尺度的依赖，采用较小的标准差即可



对于学习率来说，太小收敛过慢，太大会震荡。根据training cost来确定学习速率。

early stopping: validation accuracy，记录最佳的validation accuracy 当连续十次epoch没有达到最佳accurcy认为不再提高此时使用early stopping.或者我们不停止，改变learning rate减半处理。

正则项系数，初始设为0找到合适的learning rate之后在根据validation accuracy来调整。

对各项数据进行规范化处理，即减去平均值，除以标准差。原因：神经网络假设输入和输出结构满足标准差近于1，平均值为0

### 12 表示词的方法

one-hot：每个词用one-hot形式表示，造成句子过于稀疏

bow: 忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的。length of sentence: length of vocabulary

word2vec：将语言表示为稠密、低维、连续的向量，通过训练时考虑上下文的影响。

tf-idf: tf:某个关键词在整篇文章中出现的频率,idf表示计算倒文本频率。文本频率是指某个关键词在整个语料所有文章中出现的次数. length of sentence is the length of vocabulary.

神经词袋模型：简单对文本序列中每个词嵌入进行平均/加总，作为整个序列的表示。对长文本这种方式比较有效。

fasttext: 神经词袋+这些词的n-gram特征叠加平均得到句向量。

textcnn:\[length\_of\_words, word embedding\],对这个矩阵跑多个卷积核，然后将几个结果拼接在一起用以表示句子

textRNN:用bilstm所有隐藏层拼接之后的向量表示原有的句向量。

CNN：在一个上头反复加卷积核

### 13 RNN, HMM,MEMM, CRF区别

马尔可夫假设：句子的概率是带预测单词之前长度为n的窗口

y是label，前后有关联的序列化标签，可以把监督学习问题分为时间序列和非时间序列问题。带有前后顺序关系的序列就叫做时间序列建模

```text
f(X(t1),X(t2),X(t3),X(t4)...X(t))->Y(1),...Y(t)
```

HMM:隐藏状态分布基于离散分布，每个隐藏层都仅依赖于前一个层，下一个输出之间相互独立仅依赖当前隐藏层。参数概率用频率最大似然估计，没有出现的项用拉普拉斯平滑。做序列标注时实际为下面的等式：

```text
P(w1,..wn)=P(s|start)P(w1|s)P(b|s)P(w|b)
```

MEMM:只考虑了第一个假设，每个隐藏层仅依赖于前一层，所以还会有标注偏移的问题，可以任意选择特征，对转移概率和表现概率建立联合概率。

CRF，统计全局概率，考虑数据在全局的分布，但是不能处理长期依赖

RNN: 没有马尔可夫假设，隐藏层依赖于前面所有的隐藏层。

### 98 redis

MongoDB做高性能数据库，Redis做缓存，HBase做大数据分析。MongoDB还无法取代关系型数据库。

mysql是持久化存储，存放在磁盘里面，检索的话，会涉及到一定的IO，为了解决这个瓶颈，于是出现了缓存

1. 使用Redis有哪些好处？

\(1\) 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O\(1\)

\(2\) 支持丰富数据类型，支持string，list，set，sorted set，hash

\(3\) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行

\(4\) 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除

mongodb是文档存储数据库，支持二级索引，但比较消耗内存，查询功能强大，类似json格式存储，一般可以用来存放评论等半结构化数据redis是KV数据库，不支持二级索引，读写性能高，支持list，set等多种数据格式，适合读多写少的业务场景，可以用来做缓存系统hbase是列数据库，不支持二级索引，写性能高，适合写多读少的业务场景，可用来存储BI数据

### 98 分布式存储

HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。Hadoop使用分布式文件系统，用于存储大数据，并使用MapReduce来处理。Hadoop擅长于存储各种格式的庞大的数据，任意的格式甚至非结构化的处理。HBase中的所有数据文件都存储在Hadoop HDFS文件系统上。内部有两种格式，1. HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile 2. HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File

HDFS:
Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS设计成能可靠地在集群中大量机器之间存储大量的文件，它以块序列的形式存储文件。文件中除了最后一个块，其他块都有相同的大小。属于文件的块为了故障容错而被复制。块的大小和复制数是以文件为单位进行配置的，应用可以在文件创建时或者之后修改复制因子。HDFS中的文件是一次写的，并且任何时候都只有一个写操作。

缺点：高延时访问，HDFS不太适合于那些要求低延时（数十毫秒）访问的应用程序，因为HDFS是设计用于大吞吐量数据的，这是以一定延时为代价的。HDFS是单Master的，所有的对文件的请求都要经过它，当请求多时，肯定会有延时。
使用缓存或多master设计可以降低client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延时了。
不适合大量小文件。因为Namenode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由Namenode的内存大小来决定。一般来说，每一个文件、文件夹和Block需要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB内存。

### 99 分布式存储系统

Hadoop

MapReduce含有一个JobTracker，针对每一个集群节点都有一个TaskTracker用以分配任务。
YARN的基本思想是将资源管理和作业调度/监控的功能分为独立的守护进程。分别是一个全局的 ResourceManager（RM） 和每个应用程序的 ApplicationMaster（AM）ResourceManager负责系统中的所有应用程序的资源分配。NodeManager负责每台机器中容器代理、资源监控（cpu，内存，磁盘，网络），并将这些情况报告给ResourceManager或Scheduler。YARN运行的每个应用程序都会有一个ApplicationMaster。负责协调来自ResourceManager的资源，并通过NodeManager监控容器和资源使用（包括内存、CPU等）
过程：
1.客户端向ResourceManager提交应用程序，其中包括ApplicationMaster、启动ApplicationMaster的命令、用户程序等；
2.ResourceManager为该应用程序分配第一个Container，并与对应NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster；
3.ApplicationMaster向ResourceManager注册自己，启动成功后与ResourceManager保持心跳；
4.ApplicationMaster向ResourceManager申请资源；
5.申请资源成功后，由ApplicationMaster进行初始化，然后与NodeManager通信，要求NodeManager启动Container。然后ApplicationMaster与NodeManager保持心跳，从而对NodeManager上运行的任务进行监控和管理；
6.Container运行期间，向ApplicationMaster汇报自己的进度和状态信息，以便ApplicationMaster掌握任务运行状态，从而在任务失败是可以重新启动；
7.应用运行结束后，ApplicationMaster向ResourceManager注销自己，允许其所属的Container回收。

SPARK

RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。RDD将操作分为两类：transformation与action。无论执行了多少次transformation操作，RDD都不会真正执行运算，只有当action操作被执行时，运算才会触发。Transformation转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD，而action行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）

dependencies:建立RDD的依赖关系，主要rdd之间是宽窄依赖的关系，具有窄依赖关系的rdd可以在同一个stage中进行计算。要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD
partition：一个rdd会有若干个分区，分区的大小决定了对这个rdd计算的粒度，每个rdd的分区的计算都在一个单独的任务中进行。
preferedlocations:按照“移动数据不如移动计算”原则，在spark进行任务调度的时候，优先将任务分配到数据块存储的位置
compute：spark中的计算都是以分区为基本单位的，compute函数只是对迭代器进行复合，并不保存单次计算的结果。
partitioner：只存在于（K,V）类型的rdd中，非（K,V）类型的partitioner的值就是None。

DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格

基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。

cache和persist的区别，两者都是为了提高复用，将一个RDD进行缓存。cache 调用了persist，默认MEMORY\_ONLY, persist可以根据情况设置缓存级别。当这个RDD将多次使用时，可以考虑使用这两者

DataFrame join: inner相当于merge, left 保留左表全体，若右表中无对应元素，右表column设置为null，right与left同理，full为保留两者全体. 

{% code-tabs %}
{% code-tabs-item title="join example" %}
```text
Dataframe1.join(Dataframe2,join condition,join Type)

```
{% endcode-tabs-item %}
{% endcode-tabs %}

Union: 将多个RDD合并，不去重

利用spark sql时，可以直接通过distinct进行去重，相当于pandas的drop\_duplicated

withColumn\(colName, col\): 相当于pandas中的apply函数，col中为要进行的转化操作，如果该操作为用户自定义的函数操作，则需使用udf函数封装一下

Hadoop和Spark的区别：  
Hadoop:分布式批处理计算，强调批处理，常用于数据挖掘和数据分析。Hadoop 是磁盘级计算,进行计算时,数据在磁盘上,需要读写磁盘;Storm是内存级计算,数据直接通过网络导入内存。读写内存比读写磁盘速度快 N 个 数量级。

Spark:是一个基于内存计算的开源的集群计算系统，目的是让数据分析更加快速, Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。微批处理，每次处理的都是一批非常小的数据


虽然 Spark 与 Hadoop 有相似之处，但它提供了具有有用差异的一个新的集群计算框架。首先，Spark 是为集群计算中的特定类型的工作负载而设计，即那些在并行操作之间重用工作数据集（比如机器学习算法）的工作负载。为了优化这些类型的工作负载，Spark 引进了内存集群计算的概念，可在内存集群计算中将数据集缓存在内存中，以缩短访问延迟。

在大数据处理方面相信大家对hadoop已经耳熟能详，基于GoogleMap/Reduce来实现的Hadoop为开发者提供了map、reduce原语，使并行批处理程序变得非常地简单和优美。Spark提供的数据集操作类型有很多种，不像Hadoop只提供了Map和Reduce两种操作。比如map,filter, flatMap,sample, groupByKey, reduceByKey, union,join, cogroup,mapValues, sort,partionBy等多种操作类型，他们把这些操作称为Transformations。同时还提供Count,collect, reduce, lookup, save等多种actions。这些多种多样的数据集操作类型，给上层应用者提供了方便。

实数据处理逻辑的代码非常简短。基于RDD的抽象，它将数据存储在内存中，从而提供了低延迟性，通过将流拆成小的batch提供Discretized Stream处理流数据。中间结果放在内存中，内存放不下了会写入本地磁盘，而不是HDFS。

### 100 Docker的作用

Docker 是一个开源项目，这个项目旨在通过把应用程序打包为可移植的、自给自足的容器。DocKer一次构建可放在任何地方就可以运行，不需要进行任何改变DocKer 就类似于一个容器。多个容器共享操作系统的内核
1）DocKer 启动速度快
2）在资源的利用上也比虚机高多了
